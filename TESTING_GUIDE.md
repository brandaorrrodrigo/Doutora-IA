# GUIA DE TESTES - Doutora IA

Suite completa de testes automatizados com pytest, cobertura de cÃ³digo e CI/CD.

---

## ðŸŽ¯ Cobertura de Testes

### MÃ³dulos Testados

âœ… **AutenticaÃ§Ã£o (test_auth.py)**
- Registro de usuÃ¡rios
- Login/logout
- ValidaÃ§Ã£o de JWT tokens
- Hashing de senhas
- Refresh tokens

âœ… **API Endpoints (test_api_endpoints.py)**
- Health check
- RAG search
- AnÃ¡lise de casos
- GeraÃ§Ã£o de relatÃ³rios PDF
- ComposiÃ§Ã£o de peÃ§as DOCX
- Webhooks de pagamento
- Endpoints de advogados
- Seed de banco de dados

âœ… **ServiÃ§os (test_services.py)**
- RAG system (Qdrant)
- Payment service (stub, Mercado Pago, Binance, Stripe)
- PDF generation
- DOCX composition
- Citation extraction
- Auth service (JWT, bcrypt)

---

## ðŸš€ Executando os Testes

### OpÃ§Ã£o 1: Script AutomÃ¡tico (Recomendado)

#### Linux/Mac:
```bash
./scripts/run_tests.sh
```

#### Windows PowerShell:
```powershell
.\scripts\run_tests.ps1
```

### OpÃ§Ã£o 2: Pytest Direto

```bash
cd api

# Instalar dependÃªncias de teste
pip install -r requirements-test.txt

# Rodar todos os testes
pytest

# Rodar com cobertura
pytest --cov=. --cov-report=html

# Rodar testes especÃ­ficos
pytest tests/test_auth.py
pytest tests/test_api_endpoints.py::TestHealthEndpoint

# Rodar em modo verbose
pytest -v

# Rodar apenas testes rÃ¡pidos (excluir slow)
pytest -m "not slow"
```

---

## ðŸ“Š Cobertura de CÃ³digo

### Meta: 70%+ cobertura

ApÃ³s rodar os testes, visualize o relatÃ³rio de cobertura:

```bash
# Abrir relatÃ³rio HTML (gerado em api/htmlcov/)
open api/htmlcov/index.html  # Mac
xdg-open api/htmlcov/index.html  # Linux
start api\htmlcov\index.html  # Windows
```

### Verificar cobertura por arquivo:

```bash
pytest --cov=. --cov-report=term-missing
```

SaÃ­da exemplo:
```
Name                          Stmts   Miss  Cover   Missing
-----------------------------------------------------------
main.py                         156      8    95%   45-47, 89
models.py                        42      0   100%
services/auth.py                 38      2    95%   67-68
services/payments.py            124     15    88%   102-115
services/compose_docx.py         89      5    94%   78-82
rag.py                          112     18    84%   145-162
-----------------------------------------------------------
TOTAL                           561     48    91%
```

---

## ðŸ§ª Tipos de Testes

### 1. Testes UnitÃ¡rios

Testam funÃ§Ãµes individuais isoladamente:

```python
def test_hash_password():
    from services.auth import hash_password

    password = "my_password_123"
    hashed = hash_password(password)

    assert hashed != password
    assert len(hashed) > 0
```

### 2. Testes de IntegraÃ§Ã£o

Testam fluxos completos da API:

```python
def test_analyze_case_success(client, mock_rag_results):
    response = client.post(
        "/analyze_case",
        json={
            "descricao": "Sofri fraude PIX...",
            "detalhado": False
        }
    )

    assert response.status_code == 200
    data = response.json()
    assert "tipificacao" in data
```

### 3. Testes com Mocks

Simulam dependÃªncias externas (Qdrant, LLM, etc.):

```python
@patch("rag.rag_system.search")
@patch("main.llm_client.chat.completions.create")
def test_with_mocks(mock_llm, mock_search, client):
    mock_search.return_value = [...]
    mock_llm.return_value = ...

    # Test code here
```

---

## ðŸ·ï¸ Marcadores (Markers)

Use marcadores para categorizar testes:

```python
@pytest.mark.slow
def test_heavy_operation():
    # Teste demorado
    pass

@pytest.mark.integration
def test_full_flow():
    # Teste de integraÃ§Ã£o
    pass

@pytest.mark.requires_qdrant
def test_rag_search():
    # Requer Qdrant rodando
    pass
```

Executar apenas testes marcados:

```bash
# Rodar apenas testes rÃ¡pidos
pytest -m "not slow"

# Rodar apenas testes de integraÃ§Ã£o
pytest -m integration

# Rodar apenas testes unitÃ¡rios
pytest -m unit
```

---

## ðŸ”§ Fixtures

Fixtures sÃ£o recursos reutilizÃ¡veis entre testes:

```python
@pytest.fixture
def test_user(db_session):
    """Cria um usuÃ¡rio de teste"""
    user = models.User(
        email="test@example.com",
        password_hash=hash_password("password123")
    )
    db_session.add(user)
    db_session.commit()
    return user

def test_login(client, test_user):
    # Usa a fixture test_user
    response = client.post(
        "/auth/login",
        params={
            "email": test_user.email,
            "password": "password123"
        }
    )
    assert response.status_code == 200
```

### Fixtures DisponÃ­veis:

- `client`: TestClient do FastAPI
- `db_session`: SessÃ£o de banco de dados (SQLite in-memory)
- `test_user`: UsuÃ¡rio de teste
- `test_lawyer`: Advogado de teste
- `auth_headers`: Headers de autenticaÃ§Ã£o JWT
- `test_case`: Caso jurÃ­dico de teste
- `test_report`: RelatÃ³rio de teste
- `mock_rag_results`: Resultados simulados do RAG
- `mock_llm_response`: Resposta simulada do LLM

---

## ðŸ“ Escrevendo Novos Testes

### Template de Teste:

```python
"""
Tests for [module name]
"""

import pytest
from unittest.mock import patch


class Test[ModuleName]:
    """Test [functionality]"""

    def test_[scenario]_success(self, client):
        """Test [scenario] succeeds"""
        response = client.post("/endpoint", json={...})

        assert response.status_code == 200
        data = response.json()
        assert data["field"] == "expected_value"

    def test_[scenario]_failure(self, client):
        """Test [scenario] fails correctly"""
        response = client.post("/endpoint", json={...})

        assert response.status_code == 400
        assert "error message" in response.json()["detail"]
```

### Boas PrÃ¡ticas:

1. **Nomes descritivos**: `test_login_with_invalid_password`
2. **Uma asserÃ§Ã£o por teste** (quando possÃ­vel)
3. **Arrange-Act-Assert** (AAA pattern):
   ```python
   # Arrange (preparar)
   user = create_test_user()

   # Act (executar)
   response = client.post("/login", ...)

   # Assert (verificar)
   assert response.status_code == 200
   ```
4. **Isolar testes**: Cada teste deve ser independente
5. **Usar fixtures**: Reutilizar cÃ³digo de setup
6. **Mockar dependÃªncias externas**: LLM, Qdrant, pagamentos

---

## ðŸ› Debugging Testes

### Rodar um teste especÃ­fico com debug:

```bash
# Rodar com print statements visÃ­veis
pytest -s tests/test_auth.py::TestAuthEndpoints::test_login_success

# Rodar com debugger (pdb)
pytest --pdb tests/test_auth.py

# Ver traceback completo
pytest --tb=long tests/test_auth.py
```

### Logging em testes:

```python
import logging

def test_with_logging(caplog):
    with caplog.at_level(logging.INFO):
        # CÃ³digo que gera logs
        result = some_function()

    # Verificar logs
    assert "Expected log message" in caplog.text
```

---

## ðŸ“ˆ MÃ©tricas de Qualidade

### Coverage Badge (GitHub)

```markdown
![Coverage](https://img.shields.io/badge/coverage-91%25-brightgreen)
```

### CritÃ©rios de Qualidade:

- âœ… Cobertura >= 70%
- âœ… Todos os testes passando
- âœ… Sem warnings de deprecaÃ§Ã£o
- âœ… Tempo de execuÃ§Ã£o < 2 minutos (todos os testes)
- âœ… Todos os endpoints crÃ­ticos testados

---

## ðŸ”„ IntegraÃ§Ã£o CI/CD

Os testes rodam automaticamente em:

### GitHub Actions:

```yaml
# .github/workflows/test.yml
- name: Run tests
  run: |
    pip install -r api/requirements-test.txt
    cd api && pytest --cov=. --cov-report=xml

- name: Upload coverage
  uses: codecov/codecov-action@v3
```

### Pre-commit Hook (opcional):

```bash
# Rodar testes antes de commit
cat > .git/hooks/pre-commit << 'EOF'
#!/bin/bash
./scripts/run_tests.sh
EOF

chmod +x .git/hooks/pre-commit
```

---

## ðŸŽ¯ Testes por Funcionalidade

### AutenticaÃ§Ã£o:
- âœ… Registro de usuÃ¡rio
- âœ… Login
- âœ… ValidaÃ§Ã£o de token
- âœ… Refresh token
- âœ… ProteÃ§Ã£o de rotas

### RAG Search:
- âœ… Busca bÃ¡sica
- âœ… Busca com filtros (Ã¡rea, tipo)
- âœ… Ranking hierÃ¡rquico
- âœ… FormataÃ§Ã£o de contexto

### AnÃ¡lise de Casos:
- âœ… AnÃ¡lise simples
- âœ… AnÃ¡lise detalhada
- âœ… ExtraÃ§Ã£o de citaÃ§Ãµes
- âœ… Parsing de resposta LLM

### Pagamentos:
- âœ… CriaÃ§Ã£o de pagamento (stub)
- âœ… Webhook verification
- âœ… Multi-provider (MP, Binance, Stripe)
- âœ… Signature validation

### Documentos:
- âœ… GeraÃ§Ã£o de PDF
- âœ… ComposiÃ§Ã£o de DOCX
- âœ… Template rendering
- âœ… CitaÃ§Ãµes incorporadas

---

## ðŸ” Troubleshooting

### Tests failing with "ModuleNotFoundError"

```bash
# Instalar em modo development
cd api
pip install -e .
```

### Tests failing with "Database connection error"

```bash
# Verificar que estÃ¡ usando SQLite in-memory (configurado em conftest.py)
# Se necessÃ¡rio, limpar banco de dados:
rm -f test.db
```

### Coverage nÃ£o estÃ¡ sendo calculada

```bash
# Reinstalar pytest-cov
pip uninstall pytest-cov
pip install pytest-cov
```

### Tests muito lentos

```bash
# Rodar apenas testes rÃ¡pidos
pytest -m "not slow"

# Rodar em paralelo (pytest-xdist)
pip install pytest-xdist
pytest -n auto
```

---

## âœ… Checklist de Qualidade

Antes de fazer commit/deploy:

- [ ] Todos os testes passando (`pytest`)
- [ ] Cobertura >= 70% (`pytest --cov`)
- [ ] Sem warnings (`pytest --strict-warnings`)
- [ ] Linter passando (`flake8 .`)
- [ ] Type checking (opcional: `mypy .`)
- [ ] DocumentaÃ§Ã£o atualizada
- [ ] Changelog atualizado

---

## ðŸ“š Recursos

- **Pytest Docs**: https://docs.pytest.org/
- **Coverage.py**: https://coverage.readthedocs.io/
- **FastAPI Testing**: https://fastapi.tiangolo.com/tutorial/testing/
- **Pytest Fixtures**: https://docs.pytest.org/en/stable/fixture.html

---

**Suite de testes pronta para produÃ§Ã£o!** ðŸŽ‰

Cobertura de 70%+, testes automÃ¡ticos e CI/CD integrado.

# OLLAMA SETUP - Doutora IA

Guia rÃ¡pido para usar **Ollama** (100% local, grÃ¡tis, privado) no lugar do OpenAI.

---

## âœ… JÃ ESTÃ CONFIGURADO!

O arquivo `api/.env` jÃ¡ estÃ¡ configurado para usar **Ollama**:

```bash
LLM_BASE_URL=http://host.docker.internal:11434/v1
OPENAI_API_KEY=ollama
LLM_MODEL=llama3:latest
```

---

## ğŸš€ COMO TESTAR

### 1. Garantir que Ollama estÃ¡ rodando

```bash
# Ver modelos disponÃ­veis
ollama list

# Deve mostrar:
# llama3:latest      (4.7 GB)
# gpt-oss:20b       (13 GB)
# deepseek-r1:8b    (5.2 GB)
```

**Ollama roda automaticamente em background**, mas se precisar iniciar manualmente:

```bash
# Windows
ollama serve

# Deixar rodando
```

---

### 2. Testar Ollama diretamente

```bash
# Teste simples
ollama run llama3:latest "OlÃ¡, vocÃª Ã© um assistente jurÃ­dico. Explique o que Ã© LGPD em 2 frases."

# Se funcionar, Ollama estÃ¡ OK!
```

---

### 3. Iniciar Doutora IA

```bash
cd C:\Users\NFC\doutora-ia

# Iniciar serviÃ§os
docker-compose up -d

# Ver logs
docker-compose logs -f api
```

---

### 4. Testar AnÃ¡lise com Ollama

```bash
# Fazer anÃ¡lise de caso
curl -X POST http://localhost:8000/analyze_case \
  -H "Content-Type: application/json" \
  -d '{
    "descricao": "Cliente sofreu acidente de trÃ¢nsito e teve lesÃµes graves. O outro motorista estava embriagado. Deseja processar por danos morais e materiais.",
    "detalhado": false
  }'

# VocÃª verÃ¡ nos logs:
# "Cache MISS - Calling LLM ($$): Cliente sofreu acidente..."
# E a resposta virÃ¡ do OLLAMA (nÃ£o OpenAI)!
```

---

## ğŸ”„ TROCAR DE MODELO

Quer testar com um modelo diferente? Ã‰ sÃ³ editar `.env`:

```bash
# Usar GPT-OSS (20B - melhor qualidade, mais lento)
LLM_MODEL=gpt-oss:20b

# Usar DeepSeek R1 (8B - excelente para raciocÃ­nio)
LLM_MODEL=deepseek-r1:8b

# Depois reiniciar
docker-compose restart api
```

---

## ğŸ“Š COMPARAÃ‡ÃƒO DE MODELOS

### Llama 3 (4.7 GB) - PADRÃƒO
- âœ… **Velocidade**: RÃ¡pido (5-10s por anÃ¡lise)
- âœ… **Qualidade**: Boa
- âœ… **MemÃ³ria**: 8 GB RAM suficiente
- ğŸ‘ **Recomendado para**: Desenvolvimento, testes rÃ¡pidos

### GPT-OSS 20B (13 GB)
- âš ï¸ **Velocidade**: Lento (20-40s por anÃ¡lise)
- âœ… **Qualidade**: Excelente
- âš ï¸ **MemÃ³ria**: 16+ GB RAM recomendado
- ğŸ‘ **Recomendado para**: AnÃ¡lises complexas, produÃ§Ã£o

### DeepSeek R1 8B (5.2 GB)
- âœ… **Velocidade**: MÃ©dio (10-15s por anÃ¡lise)
- âœ… **Qualidade**: Muito boa (especializado em raciocÃ­nio)
- âœ… **MemÃ³ria**: 10 GB RAM suficiente
- ğŸ‘ **Recomendado para**: AnÃ¡lises jurÃ­dicas detalhadas

---

## ğŸ’° ECONOMIA COM OLLAMA

### Custos com OpenAI:
- 1000 anÃ¡lises/dia
- ~2000 tokens por anÃ¡lise
- gpt-4o-mini: $0.150 por 1M tokens input
- **Custo: ~$600/mÃªs** ğŸ’¸

### Custos com Ollama:
- âˆ anÃ¡lises/dia
- 0 tokens pagos
- **Custo: R$ 0/mÃªs** âœ…

**Economia: 100%!** ğŸ‰

---

## âš¡ PERFORMANCE

### Cache ainda funciona!

O cache Redis **economiza processamento** mesmo com Ollama local:

- **Sem cache**: Toda anÃ¡lise = 5-10s de processamento Ollama
- **Com cache**: AnÃ¡lises repetidas = < 50ms (do Redis)

**Cache Ã© ainda MAIS importante com Ollama** porque economiza processamento local!

```bash
# Ver stats de cache
curl http://localhost:8000/cache/stats

# Response:
{
  "cache": {
    "hit_rate": "73.5%",
    "llm_calls_saved": 1523,
    "message": "Saved 1523 local LLM calls"
  }
}
```

---

## ğŸ› TROUBLESHOOTING

### Erro: "Connection refused localhost:11434"

**Causa**: Ollama nÃ£o estÃ¡ rodando ou Docker nÃ£o consegue acessar host

**SoluÃ§Ã£o 1** - Verificar Ollama:
```bash
ollama list  # Se der erro, Ollama estÃ¡ off

# Iniciar Ollama
ollama serve
```

**SoluÃ§Ã£o 2** - Usar IP do host (se host.docker.internal nÃ£o funcionar):
```bash
# Descobrir IP do host
ipconfig

# Exemplo: 192.168.1.100
# Editar api/.env:
LLM_BASE_URL=http://192.168.1.100:11434/v1
```

---

### Erro: "Model not found"

**Causa**: Modelo nÃ£o estÃ¡ baixado

**SoluÃ§Ã£o**:
```bash
# Baixar modelo
ollama pull llama3:latest

# Ou o modelo que vocÃª quer usar
ollama pull deepseek-r1:8b
```

---

### Ollama estÃ¡ lento

**Causa**: Modelo muito grande ou CPU fraco

**SoluÃ§Ãµes**:
- Usar modelo menor: `llama3:latest` (4.7 GB) em vez de `gpt-oss:20b` (13 GB)
- Aumentar cache hit rate para reduzir chamadas ao Ollama
- Usar GPU se disponÃ­vel (Ollama detecta automaticamente)

---

### Como verificar se estÃ¡ usando Ollama (nÃ£o OpenAI)

```bash
# Ver logs da API
docker-compose logs -f api

# Fazer anÃ¡lise
curl -X POST http://localhost:8000/analyze_case ...

# Nos logs vocÃª verÃ¡:
# âœ… Se estiver usando Ollama: requests para localhost:11434
# âŒ Se estiver usando OpenAI: requests para api.openai.com

# Ou verificar terminal onde ollama estÃ¡ rodando
# VocÃª verÃ¡ logs de requests chegando
```

---

## ğŸ¯ MODO HÃBRIDO (AvanÃ§ado)

Quer usar **Ollama para dev** e **OpenAI para produÃ§Ã£o**?

### Configurar por ambiente:

```python
# api/main.py - Adicionar lÃ³gica:

import os

ENV = os.getenv("ENV", "development")

if ENV == "production":
    # ProduÃ§Ã£o: OpenAI (melhor qualidade, custo)
    llm_base_url = "https://api.openai.com/v1"
    llm_model = "gpt-4o-mini"
    llm_api_key = os.getenv("OPENAI_API_KEY")
else:
    # Dev: Ollama (grÃ¡tis, local)
    llm_base_url = "http://host.docker.internal:11434/v1"
    llm_model = "llama3:latest"
    llm_api_key = "ollama"

llm_client = OpenAI(base_url=llm_base_url, api_key=llm_api_key)
```

Ou simplesmente trocar `.env` ao fazer deploy! ğŸš€

---

## âœ… RESUMO

**ConfiguraÃ§Ã£o atual:**
- âœ… Ollama instalado
- âœ… 3 modelos disponÃ­veis (llama3, gpt-oss, deepseek-r1)
- âœ… `.env` configurado para usar `llama3:latest`
- âœ… Cache Redis habilitado (economiza processamento)
- âœ… 100% grÃ¡tis, 100% privado, 100% local

**Para testar:**
```bash
# 1. Garantir Ollama rodando
ollama list

# 2. Iniciar Doutora IA
cd C:\Users\NFC\doutora-ia
docker-compose up -d

# 3. Testar anÃ¡lise
curl -X POST http://localhost:8000/analyze_case -H "Content-Type: application/json" -d '{"descricao": "Caso de teste", "detalhado": false}'

# 4. Profit! ğŸ‰
```

**Quer voltar para OpenAI?**
```bash
# Editar api/.env:
LLM_BASE_URL=https://api.openai.com/v1
OPENAI_API_KEY=sua_key_aqui
LLM_MODEL=gpt-4o-mini

# Reiniciar
docker-compose restart api
```

---

**Sistema 100% flexÃ­vel! Use Ollama grÃ¡tis ou OpenAI quando precisar!** ğŸš€
